Great! Let‚Äôs continue with:

---

## üîπ **2.2.3 Pulling GPU-Compatible Models in Ollama**

---

### ‚úÖ 1. What Is This Step?

This step is about:

* Downloading LLMs that are **compatible with GPU**
* Making sure Ollama pulls models in the **GGUF format** optimized for inference
* Ensuring models run on **CUDA-enabled GPU** if available

---

### ‚úÖ 2. Why Do We Need to Pull Models?

* üß† Ollama doesn‚Äôt come with models pre-installed
* You must **pull models manually** using the `ollama pull` command
* Different models have different size versions optimized for CPU or GPU (e.g., 3B, 7B, 13B)

---

### ‚úÖ 3. When Should You Pull GPU-Compatible Models?

* After installing Ollama and verifying CUDA is working
* When you plan to run the model for **chat, embedding, reranking, or RAG**
* Anytime you want to **experiment with a new model** or **upgrade**

---

### ‚úÖ 4. Where Are the Models Stored?

Models are stored in:

```
~/.ollama/models/
```

You can list all pulled models:

```bash
ollama list
```

They are stored locally, and Ollama **does not re-download** them each time.

---

### ‚öôÔ∏è Pulling a Model (Command)

```bash
ollama pull mistral
```

This downloads the default quantized GGUF version of Mistral (usually `q4_0` or `q4_K_M`).

Other examples:

```bash
ollama pull llama2
ollama pull codellama
ollama pull phi
```

üîç To view available models:
Visit [https://ollama.com/library](https://ollama.com/library)

---

### üß™ Test: Run the Model

```bash
ollama run mistral
```

If everything is okay, Ollama will use the GPU automatically.

Behind the scenes:

* Uses **llama.cpp** with GPU acceleration
* Loads the GGUF model file
* Streams output through `localhost:11434`

---

### ‚ö†Ô∏è Tip: Choose GPU-Friendly Quantization

Ollama defaults to **quantized formats**. For best GPU usage:

* Use `q4_K_M` or higher
* Use `OLLAMA_NUM_GPU_LAYERS` to shift layers to GPU:

```bash
OLLAMA_NUM_GPU_LAYERS=35 ollama run mistral
```

---

### üìå Optional: Create Custom Model

You can create your own model from a base:

```bash
ollama create mymodel -f Modelfile
```

Sample `Modelfile`:

```
FROM mistral
SYSTEM You are a helpful AI assistant.
```

Then:

```bash
ollama run mymodel
```

---

### üìã Summary Table

| Aspect        | Explanation                                                    |
| ------------- | -------------------------------------------------------------- |
| What is it?   | Downloading GGUF format LLMs for local inference               |
| Why needed?   | Ollama doesn‚Äôt include models by default; pulling is necessary |
| When to pull? | After installation and before LangChain integration            |
| Where stored? | `~/.ollama/models/`                                            |
| Common models | `mistral`, `llama2`, `phi`, `codellama`, `dolphin-mixtral`     |
| GPU tips      | Use quantized formats + set `OLLAMA_NUM_GPU_LAYERS`            |

---

‚úÖ Shall we proceed to **2.2.4 Running Ollama Inside vs Outside Docker (pros and cons)**?
