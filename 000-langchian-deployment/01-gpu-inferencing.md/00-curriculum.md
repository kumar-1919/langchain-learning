# ðŸš€ Section 2: GPU Inference using Ollama / HuggingFace Models

---

LangChain can use **local or remote models** for LLM tasks like embedding, generation, and reranking.

In this section, weâ€™ll focus on **running models locally** using your **GPU** to accelerate performance using:

### ðŸ”¹ Ollama â€“ Lightweight, containerized model runner

### ðŸ”¹ HuggingFace Transformers â€“ Open-source LLMs with GPU acceleration

---

## ðŸ“˜ Section 2 Curriculum (Subtopics)

Weâ€™ll go step by step, one sub-subtopic at a time:

### ðŸ”¹ 2.1 Understanding GPU Inference

* âœ… What is GPU inference?
* âœ… Why use GPU instead of CPU?
* âœ… When and where is GPU inference useful in LangChain?

### ðŸ”¹ 2.2 GPU Inference with Ollama

* 2.2.1 What is Ollama and how it works
* 2.2.2 Installing Ollama with GPU support (NVIDIA)
* 2.2.3 Pulling GPU-compatible models
* 2.2.4 Running Ollama inside vs outside Docker (pros/cons)
* 2.2.5 LangChain integration with Ollama
* 2.2.6 Performance tips and memory management

### ðŸ”¹ 2.3 GPU Inference with HuggingFace Transformers

* 2.3.1 What are Transformers and model types (causal, encoder-decoder)
* 2.3.2 Installing HuggingFace + `torch` with CUDA
* 2.3.3 Choosing GPU-supported models
* 2.3.4 Running models on GPU manually
* 2.3.5 LangChain integration with HuggingFace LLMs

---